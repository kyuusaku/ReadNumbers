{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'Const:0' shape=() dtype=float32>, <tf.Tensor 'Const_1:0' shape=() dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notice that printing the nodes does not output the values 3.0 and 4.0 as you might expect. \n",
    "Instead, they are nodes that, when evaluated, would produce 3.0 and 4.0, respectively. \n",
    "To actually evaluate the nodes, we must run the computational graph within a session. \n",
    "A session encapsulates the control and state of the TensorFlow runtime.\n",
    "\"\"\"\n",
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('node3: ', <tf.Tensor 'Add:0' shape=() dtype=float32>)\n",
      "('sess.run(node3): ', 7.0)\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3: \", node3)\n",
    "print(\"sess.run(node3): \",sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A graph can be parameterized to accept external inputs, known as placeholders. \n",
    "A placeholder is a promise to provide a value later.\n",
    "\"\"\"\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b:4.5}))\n",
    "print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. \n",
    "Variables allow us to add trainable parameters to a graph. They are constructed with a type and initial value:\n",
    "\"\"\"\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A variable is initialized to the value provided to tf.Variable \n",
    "but can be changed using operations like tf.assign\n",
    "\"\"\"\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In general, computing symbolic derivatives manually is tedious and error-prone. \n",
    "Consequently, TensorFlow can automatically produce derivatives given only a description \n",
    "of the model using the function tf.gradients. \n",
    "For simplicity, optimizers typically do this for you.\n",
    "\"\"\"\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x144bf8550>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmp7s4XRX\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/suo/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmp7s4XRX/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.25, step = 1\n",
      "INFO:tensorflow:global_step/sec: 330.539\n",
      "INFO:tensorflow:loss = 0.00745719, step = 101 (0.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 311.781\n",
      "INFO:tensorflow:loss = 0.00824416, step = 201 (0.320 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.297\n",
      "INFO:tensorflow:loss = 0.00192063, step = 301 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 544.229\n",
      "INFO:tensorflow:loss = 0.00053425, step = 401 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.292\n",
      "INFO:tensorflow:loss = 1.44615e-05, step = 501 (0.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 395.715\n",
      "INFO:tensorflow:loss = 7.62083e-06, step = 601 (0.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 520.706\n",
      "INFO:tensorflow:loss = 1.34705e-06, step = 701 (0.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 545.437\n",
      "INFO:tensorflow:loss = 3.07037e-07, step = 801 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 555.516\n",
      "INFO:tensorflow:loss = 2.84498e-08, step = 901 (0.180 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmp7s4XRX/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 7.19425e-10.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/suo/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-30-06:27:57\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmp7s4XRX/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-30-06:27:58\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 6.73191e-09\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /Users/suo/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-30-06:27:58\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmp7s4XRX/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-30-06:28:00\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.00253288\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "train loss: {'loss': 6.7319079e-09, 'global_step': 1000}\n",
      "eval loss: {'loss': 0.0025328849, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one real-valued feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension=1)]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# logistic regression, linear classification, logistic classification, and\n",
    "# many neural network classifiers and regressors. The following code\n",
    "# provides an estimator that does linear regression.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x_train}, y_train,\n",
    "                                              batch_size=4,\n",
    "                                              num_epochs=1000)\n",
    "eval_input_fn = tf.contrib.learn.io.numpy_input_fn(\n",
    "    {\"x\":x_eval}, y_eval, batch_size=4, num_epochs=1000)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train loss: %r\"% train_loss)\n",
    "print(\"eval loss: %r\"% eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x141858190>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmpys9EXA\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmpys9EXA/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.4263184618, step = 1\n",
      "INFO:tensorflow:global_step/sec: 509.23\n",
      "INFO:tensorflow:loss = 0.212637358206, step = 101 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 531.767\n",
      "INFO:tensorflow:loss = 0.0137006210751, step = 201 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 639.673\n",
      "INFO:tensorflow:loss = 0.00135573624776, step = 301 (0.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 658.241\n",
      "INFO:tensorflow:loss = 0.000118250636023, step = 401 (0.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 781.317\n",
      "INFO:tensorflow:loss = 2.63608752063e-05, step = 501 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 760.48\n",
      "INFO:tensorflow:loss = 1.47181304338e-06, step = 601 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 734.814\n",
      "INFO:tensorflow:loss = 1.69709042502e-07, step = 701 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.373\n",
      "INFO:tensorflow:loss = 1.22918712812e-08, step = 801 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 696.065\n",
      "INFO:tensorflow:loss = 2.84715963683e-10, step = 901 (0.144 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmpys9EXA/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.18145978586e-10.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-30-07:15:23\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmpys9EXA/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-30-07:15:24\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 7.31215e-11\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-30-07:15:25\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/q0/2tmk_x7j6ng2pb10kby0g64c0000gn/T/tmpys9EXA/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-30-07:15:26\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101011\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "train loss: {'loss': 7.312146e-11, 'global_step': 1000}\n",
      "eval loss: {'loss': 0.010101051, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Declare list of features, we only have one real-valued feature\n",
    "def model(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W*features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # ModelFnOps connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.contrib.learn.ModelFnOps(\n",
    "      mode=mode, predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_train}, y_train, 4, num_epochs=1000)\n",
    "\n",
    "# train\n",
    "estimator.fit(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did. \n",
    "train_loss = estimator.evaluate(input_fn=input_fn)\n",
    "eval_loss = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train loss: %r\"% train_loss)\n",
    "print(\"eval loss: %r\"% eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
